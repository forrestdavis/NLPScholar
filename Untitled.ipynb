{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b4da08e-f7c6-4b6a-ae3b-301decc68daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "import torch\n",
    "\n",
    "text = ['the boy is outside or so i thought', 'i hate the weather', 'the girl']\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    max_length=5,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=2,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100a3f92-7ea3-4a8a-a7bd-7de65f73c876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1169,  2933,   318,  2354,   393,   523,  1312,  1807],\n",
       "        [   72,  5465,   262,  6193, 50256, 50256, 50256, 50256],\n",
       "        [ 1169,  2576, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(text, \n",
    "                   padding=True, \n",
    "                   return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d49e5-2e96-4014-977f-20dd725e409b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac1066dd-e05f-481a-8204-de3bb941bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1169, 2933,  318, 2354,  393,  523, 1312, 1807],\n",
      "        [  72, 5465,  262, 6193, -100, -100, -100, -100],\n",
      "        [1169, 2576, -100, -100, -100, -100, -100, -100]])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "attn_mask = inputs.attention_mask\n",
    "ignore_mask = (attn_mask - 1)*100\n",
    "target_ids = (inputs.input_ids.clone() * attn_mask) + ignore_mask\n",
    "print(target_ids)\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c21e47-e2d2-42d7-80f4-c5313af8f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db516989-264f-4dfc-b7ff-bac48303fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, GPT2TokenizerFast\n",
    "\n",
    "device = \"mps\"\n",
    "model_id = \"gpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cab641da-a2e0-4770-ab9f-f93bb9b28564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0e1a263-a29b-417c-af1c-d7cb2710c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings['input_ids'] = encodings.input_ids[:,:1024]\n",
    "encodings['attention_mask'] = encodings.attention_mask[:,:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16135dba-500e-49ea-8942-80b9619b1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ede3f29-2509-4c2d-ad42-d55209dfa90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = tokenizer(['the boy is outside.'\n",
    "                       , 'I hate it']\n",
    "                      , return_tensors='pt', padding=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4065a1d7-0edd-4355-b46b-2ff4d61b60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1169,  2933,   318,  2354,    13],\n",
      "        [   72,  5465,   340, 50256, 50256]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0]], device='mps:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([[ 1169,  2933,   318,  2354],\n",
      "        [   72,  5465,   340, 50256]], device='mps:0')\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 0]], device='mps:0')\n",
      "tensor([[ 1169,  2933,   318,  2354],\n",
      "        [   72,  5465,   340, 50256]], device='mps:0')\n",
      "\n",
      "tensor([[  318,  2354,    13],\n",
      "        [  340, 50256, 50256]], device='mps:0')\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 0, 0]], device='mps:0')\n",
      "tensor([[ -100,  -100,    13],\n",
      "        [ -100,  -100, 50256]], device='mps:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_output(texts, stride=None):\n",
    "    if stride is None:\n",
    "        stride = 2\n",
    "\n",
    "    max_length = 4\n",
    "    #MAX_LENGTH = self.tokenizer.model_max_length\n",
    "\n",
    "    inputs_dict = tokenizer(texts, \n",
    "                                 padding=True,\n",
    "                                 return_tensors='pt').to(device)\n",
    "    print(inputs_dict)\n",
    "    print('-'*80)\n",
    "    seq_length = inputs_dict.input_ids.size(1)\n",
    "    logits = None\n",
    "    prev_end_loc = 0\n",
    "\n",
    "    for begin_loc in range(0, seq_length, stride):\n",
    "        # Find the end location, it's either the whole sequence \n",
    "        # if the sequence length is less than the max length or \n",
    "        # up until the max length\n",
    "        end_loc = min(begin_loc + max_length, seq_length)\n",
    "        # If the sequence ends early, it may be different than stride on last loop\n",
    "        target_length = end_loc - prev_end_loc \n",
    "        input_ids = inputs_dict.input_ids[:, begin_loc:end_loc].to(device)\n",
    "        attn_mask = inputs_dict.attention_mask[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone()\n",
    "        # Set repeated targets target to -100 \n",
    "        target_ids[:, :-target_length] = -100\n",
    "        print(input_ids)\n",
    "        print(attn_mask)\n",
    "        print(target_ids)\n",
    "        print()\n",
    "        outputs = model(**{'input_ids': input_ids, \n",
    "                         'attention_mask': attn_mask, \n",
    "                         'labels': target_ids})\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "        \n",
    "get_output(['the boy is outside.', 'i hate it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "811b3287-4095-45b4-ab9c-1b5f6c8641b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                           | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "tensor([[  628,   796,  5199,  ...,   290, 43823, 24265]], device='mps:0')\n",
      "\n",
      "[tensor(2.8187, device='mps:0')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "#max_length = 5\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "nlls = []\n",
    "logits = None\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    print(input_ids.shape)\n",
    "    print(target_ids)\n",
    "    print()\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "print(nlls)\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832f5b02-c263-41d1-81ab-90bb29ca15c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(351.5309, device='mps:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61459f45-3a22-457e-a0ff-4ba577294c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9670, device='mps:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**encodings, labels=encodings['input_ids']).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0e48813-3d4d-446b-884e-109ba17e2c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1169,  2933,   318,  2354,    13],\n",
      "        [   40,  5465,   340, 50256, 50256]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 0, 0]], device='mps:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([[ 1169,  2933,   318,  2354],\n",
      "        [   40,  5465,   340, 50256]], device='mps:0')\n",
      "tensor([[ 1169,  2933,   318,  2354],\n",
      "        [   40,  5465,   340, 50256]], device='mps:0')\n",
      "tensor([[   13],\n",
      "        [50256]], device='mps:0')\n",
      "tensor([[   13],\n",
      "        [50256]], device='mps:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[srcBuf length] > 0 INTERNAL ASSERT FAILED at \"/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/OperationUtils.mm\":387, please report a bug to PyTorch. Placeholder tensor is empty!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(target_ids)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# loss is calculated using CrossEntropyLoss which averages over valid labels\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# to the left by 1.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     neg_log_likelihood \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1348\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1348\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1351\u001b[0m     output \u001b[38;5;241m=\u001b[39m (lm_logits,) \u001b[38;5;241m+\u001b[39m transformer_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [srcBuf length] > 0 INTERNAL ASSERT FAILED at \"/Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/OperationUtils.mm\":387, please report a bug to PyTorch. Placeholder tensor is empty!"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 4\n",
    "stride = 4\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "print(encodings)\n",
    "print('-'*80)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in range(0, seq_len, stride):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    print(input_ids)\n",
    "    print(target_ids)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "print(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7da193ac-484c-45ad-801b-b27402404d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e78e70c0-28c4-4120-b40d-4a947c0074ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = encodings.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cdde991b-da40-4749-bebd-f16c23c00860",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = blah.copy()\n",
    "encodings['input_ids'] = encodings.input_ids[:, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9f8a3c64-3c43-46d8-a817-cc107fed4dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = tokenizer(['the boy is outside and i hate that he is so much'], padding=True, return_tensors='pt')\n",
    "encodings.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ed4b539-f70a-42b0-9f28-8d2c50651eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12 12 tensor([[1169, 2933,  318, 2354,  290, 1312, 5465,  326,  339,  318,  523,  881]],\n",
      "       device='mps:0') tensor([[1169, 2933,  318, 2354,  290, 1312, 5465,  326,  339,  318,  523,  881]],\n",
      "       device='mps:0') subsetted tensor([[1169, 2933,  318, 2354,  290, 1312, 5465,  326,  339,  318,  523,  881]],\n",
      "       device='mps:0')\n",
      "tensor(122.6580, device='mps:0')\n",
      "torch.Size([1, 12, 50257])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "max_length = 12\n",
    "stride = 5\n",
    "seq_len = encodingsssts.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "all_logits = None\n",
    "prev_end_loc = 0\n",
    "for begin_loc in range(0, seq_len, stride):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "    print(begin_loc, end_loc, trg_len, input_ids, target_ids, 'subsetted', target_ids[:, target_ids[0,:] != -100])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "        logits = outputs.logits[:, target_ids[0,:] != -100, :]\n",
    "        if all_logits is None:\n",
    "            all_logits = logits\n",
    "        else:\n",
    "            print(all_logits.shape, logits.shape)\n",
    "            all_logits = torch.cat((all_logits, logits), 1)\n",
    "\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())\n",
    "print(ppl)\n",
    "print(all_logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
