# Token Classification 

This experiment is used to assign labels to tokens in texts

## Train

This mode is used to train a model to assign token labels to every token in the text. 

You can find details about the parameters to be included in the config file [here](https://github.com/forrestdavis/NLPScholar/tree/main?tab=readme-ov-file#config-details-for-train)

### Data details

The data files passed into `trainfpath` and `validfpath` have to be jsonl files. Essentially each line is a dictionary with two keys: 
* `tokens`: The value of this is a list of all the tokens.
* `tags`: The value of this is a list of tags with the numerical value (i.e., the id not the label). 

You can find examples of this file format [here](https://github.com/forrestdavis/NLPScholar/tree/main/src/docs/token_classification_example)



## Evaluate

This mode is used to get predicted labels from the model for every sub-word token of every text in the evaluation data set. 

### Parameters to be included in the config: 

#### Required parameters

- `model`: the models to evaluate
- `datafpath`: File path and name for data TSV
- `predfpath`: File path for where the predictions will be saved

#### Optional parameters
- `id2label`: a mapping that specifies the mapping that the model uses to map numerical label ids to strings. If this is not provided, labels from the tokenizer of a pretrained model used (if present), or just the nummerical values are used. 
- `giveAllLabels`: If True, columns for each label are added to the prediction file along with the probability. If False, only label with maximum probability is provided. Default is False. 


### Data details

#### Input
This is the file passed into `datafpath`. 

**Required columns**
- `textid`: unique id for each text
- `text`: text
- `target`: the list of target labels. Ensure that a separate label is provided for punctuation. 

**Optional columns (used in analysis later)**
- Columns for different conditions

#### Output
This is the file that will be saved in `predfpath`. It has one row for every sub-word token. Here are the columns.


- `textid`: the ID of the text
- `model`: the models to evaluate
- `token`: the subword token
- `word`: the word the token belongs to
- `wordpos`: the position of the word in the text
- `punctuation`: boolean indicating if the token is punctuation
- `tokenizer`: the tokenizer the model used
- `target`: the target label
- `predicted`: the predicted label
- `prob`: the probability of the predicted label

## Analysis


The `analysis` mode of `TokenClassification` can be used in one of two ways: 

1. Get by-word token labels from the output of `evaluate`. (`evaluate` outputs token label for each sub-word token)

2. Get aggregate metrics (either overall or by token type)

### Parameters to be included in the config: 

#### Required parameters

- `predfpath`: File path and name for predictability TSV
- `resultsfpath`: File path and name for output TSV
- `save`: The final files that should be saved. This can be `by_word`, `by_tokentype`, and/or `by_cond`. If you want multiple files, they should be comma separated. 


#### Optional parameters
- `datafpath`: File path and name for data TSV with condition information
- `agg_type`: `first` or `max` determines which how the label is picked for the subword token. `first` picks the label assigned to the first token. `max` picks the label assigned to the token with the highest predicted value (Default is `first`)
- `ignore_punctuation`: True or False; whether or not labels assigned to punctuation are ignored (Default is True)
- `ignore`: the names of the token types that should be ignored in the aggregate outputs, separated by commas (Default is empty)
- `f_val`: the value for the f-score. (Default is 1)


### Data details

#### Input
This is the file generated by `evaluate` and passed into `predfpath`. You can optionally pass in a file into `datafpath` if you want metrics split by condition. THis file should minimally have the following columns: 

- `textid` which matches the textid in the predictions
- Any other condition files

#### Output

**`by_token`**
A TSV file with one row per word in the dataset per model. The label for the word is determined by the `agg_type` parameter.   

**`by_tokentype`**
 A TSV file with one row per each unique token type, with precision, recall and fscore for the token type for each condition and model combination. 

**`by_cond`**
A TSV file with one row for each condition and model combination, with accuracy as well as micro and macro precision, recall and fscores. 



